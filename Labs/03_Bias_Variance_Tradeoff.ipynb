{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoXcsCNZ1M9S"
      },
      "source": [
        "# Bias-Variance Dilemma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uCh4Sbs0Kvb"
      },
      "source": [
        "In this exercise session, we generate a synthetic dataset and examine the phenomenon\n",
        "of bias/variance tradeoff in different models. After that, we will analyse some technique\n",
        "to manage this tradeoff on real data. The presented procedures allow to decide\n",
        "which model, among a set of given models, is best suited for the problem analysed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZNb3dyn1jJz"
      },
      "source": [
        "## Bias-Variance Analysis\n",
        "\n",
        "At first, let us generate a dataset, so that we have a ground-truth about the real model.\n",
        "In this case, let us assume to have input $x \\in [0, 5] $ and target:\n",
        "\n",
        "\\begin{equation}\n",
        "t = f(x) + \\epsilon = 1 + \\frac{1}{2}x + \\frac{1}{10} x^{2} + \\epsilon,\n",
        "\\end{equation}\n",
        "where $\\epsilon$ is a Gaussian random variable with $\\mathbb{E}[\\epsilon]=0$ and $Var(\\epsilon) = \\sigma ^2 = 0.7^{2}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA70UCqAwI2K"
      },
      "source": [
        "import numpy as np\n",
        "n_points = 1000\n",
        "eps = 0.7\n",
        "\n",
        "def fun(x):\n",
        "  return 1 + 1/2 * x + 1/10 * x**2\n",
        "\n",
        "x = np.random.uniform(low=0, high=5, size=(n_points, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhB62Sfm3YNA"
      },
      "source": [
        "t = fun(x)\n",
        "t_noisy = t + eps * np.random.randn(n_points, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb_tpPUe3txT"
      },
      "source": [
        "After that, we consider two different linear regression models:\n",
        "\\begin{align}\n",
        "&L_1 : y(x) = a + bx \\\\\n",
        "&L_2 : y(x) = a + bx + cx^2\n",
        "\\end{align}\n",
        "If for the former one we do not need to make use of additional features, for the latter\n",
        "one we need to define the basis functions $\\phi_1(x_i) = x_i$ and $\\phi_2(x_i)=x_i^{2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVFlB67y4L7Q"
      },
      "source": [
        "phi = np.concatenate([x, x**2], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zJiJNnI4Xgh"
      },
      "source": [
        "Once we generated the inputs for both the models, we train them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpjmS_eC4Yv4"
      },
      "source": [
        "from sklearn import linear_model\n",
        "lin_model = linear_model.LinearRegression()\n",
        "lin_model.fit(x, t_noisy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQKNIiC740-A"
      },
      "source": [
        "qua_model = linear_model.LinearRegression()\n",
        "qua_model.fit(phi, t_noisy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7DJK8Kc8gZ9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(16,9))\n",
        "plt.scatter(x, t_noisy, label='true')\n",
        "\n",
        "plt.plot(x, lin_model.predict(x), label='linear model', color='red')\n",
        "plt.scatter(x, qua_model.predict(phi), label='quadratic model', color='orange')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jhLojcy6Uxr"
      },
      "source": [
        "Let us plot in the parameter space the models we estimated and the optimal ones, where\n",
        "the optimal one in the family $L_1$ is the model that\n",
        "$min_{a,b} \\int_{0}^{5}(f(x) - a - bx)^{2}dx$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WYs0Qzf6pAa"
      },
      "source": [
        "# Real and best paramseters\n",
        "real_par = np.array([1, 1/2, 1/10])\n",
        "best_lin_par = np.array([7/12, 1, 0])\n",
        "\n",
        "# Fitted parameters\n",
        "lin_c = np.array([lin_model.intercept_[0], lin_model.coef_[0][0], 0])\n",
        "qua_c = np.concatenate([qua_model.intercept_, qua_model.coef_.flatten()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K9cRgkm8YgW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure(figsize=(16, 9))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "s = 70\n",
        "\n",
        "ax.scatter(*real_par, marker='x', color='blue', s=s, label='Best Quadratic')\n",
        "ax.scatter(*best_lin_par, marker='o', color='red', s=s, label='Best Linear')\n",
        "ax.scatter(*qua_c, marker='+', color='blue', s=s, label='Fitted Quadratic')\n",
        "ax.scatter(*lin_c, marker='+', color='red', s=s, label='Fitted Linear')\n",
        "\n",
        "ax.set_xlabel('w_0')\n",
        "ax.set_ylabel('w_1')\n",
        "ax.set_zlabel('w_2')\n",
        "ax.view_init(elev=20., azim=32)\n",
        "plt.title('Parameter Space')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGuxAMkeOumk"
      },
      "source": [
        "As you can see, the parameters of the family $L_1$ lies in a 2D subspace, while the approximated quadratic models (i.e., those in $L_2$) may span over a 3D space. Hence, models from $L_2$ might coincide with the real model, i.e., $\\text{real_par}$, while linear models coming from $L_1$ will always suffer from a bias which can not be reduced to zero.\n",
        "\n",
        "If we want to compute the bias and variance of the two chosen models, we should\n",
        "iterate the estimation procedure over multiple datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChR0dQh7PV2I"
      },
      "source": [
        "# We write in a function the code we have seen above\n",
        "def sample_and_fit(n_points):\n",
        "  x = np.random.uniform(low=0, high=5, size=(n_points, 1))\n",
        "  t = fun(x)\n",
        "  t_noisy = t + eps * np.random.randn(n_points, 1)\n",
        "  lin_model = linear_model.LinearRegression()\n",
        "  lin_model.fit(x, t_noisy)\n",
        "  phi = np.concatenate([x, x**2], axis=1)\n",
        "  qua_model = linear_model.LinearRegression()\n",
        "  qua_model.fit(phi, t_noisy)\n",
        "  return lin_model, qua_model\n",
        "\n",
        "# repeats the code above for n_repetition times, and saves the results\n",
        "def multiple_sample_and_fit(n_repetitions, n_points):\n",
        "  lin_coeff = np.zeros((n_repetitions, 3))\n",
        "  qua_coeff = np.zeros((n_repetitions, 3))\n",
        "\n",
        "  for i in range(n_repetitions):\n",
        "    lin_model, qua_model = sample_and_fit(n_points)\n",
        "    # Store the results\n",
        "    lin_coeff[i, :] = np.concatenate([lin_model.intercept_, lin_model.coef_.flatten(), np.zeros(1)])\n",
        "    qua_coeff[i, :] = np.concatenate([qua_model.intercept_, qua_model.coef_.flatten()])\n",
        "  return lin_coeff, qua_coeff\n",
        "\n",
        "# plot the models coefficients in a 3D space\n",
        "def plot_models(lin_coeff, qua_coeff):\n",
        "  fig = plt.figure(figsize=(16, 9))\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "  # (only these 2 lines have changed)\n",
        "  ax.scatter(qua_coeff[:, 0], qua_coeff[:, 1], qua_coeff[:, 2], marker='.', color='blue', s=s, alpha=0.3)\n",
        "  ax.scatter(lin_coeff[:, 0], lin_coeff[:, 1], lin_coeff[:, 2], marker='.', color='red', s=s, alpha=0.3)\n",
        "\n",
        "  ax.scatter(*real_par, marker='x', color='blue', s=100)\n",
        "  ax.scatter(*best_lin_par, marker='o', color='red', s=100)\n",
        "  ax.set_xlabel('w_0')\n",
        "  ax.set_ylabel('w_1')\n",
        "  ax.set_zlabel('w_2')\n",
        "  ax.view_init(elev=20., azim=32)\n",
        "  plt.title('Parameter Space')\n",
        "  plt.grid()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB8sIWSXRwZs"
      },
      "source": [
        "We sample, fit and plot the obtained models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aspjjXu9Q-iJ"
      },
      "source": [
        "n_repetitions = 100\n",
        "n_points = 1000\n",
        "lin_coeff, qua_coeff = multiple_sample_and_fit(n_repetitions, n_points)\n",
        "plot_models(lin_coeff, qua_coeff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EQaAfOOWSw-"
      },
      "source": [
        "It is possible to see how the different trained models (represented as\n",
        "crosses and x symbols) spread around the optimal parameters.\n",
        "\n",
        "If we increase the number of repetitions, we just obtain more point in the cloud:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1E2-e1NTOhX"
      },
      "source": [
        "n_repetitions = 1000\n",
        "n_points = 1000\n",
        "lin_coeff, qua_coeff = multiple_sample_and_fit(n_repetitions, n_points)\n",
        "plot_models(lin_coeff, qua_coeff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P5g-j96WYdu"
      },
      "source": [
        "On contrary, the more the points we consider for estimation $(N \\rightarrow \\infty)$, the more the resulting parameter vectors are close to the real and the best model in the hypothesis space, depending on the hypothesis space we choose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdiL-K_5VBTP"
      },
      "source": [
        "n_repetitions = 100\n",
        "n_points = 10000\n",
        "lin_coeff, qua_coeff = multiple_sample_and_fit(n_repetitions, n_points)\n",
        "plot_models(lin_coeff, qua_coeff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUYkb1rjXNrt"
      },
      "source": [
        "Remember that even if we the estimated model coincides with the best model (e.g., by considering an infinite number of samples), we are not able to reduce the error on newly seen points to zero due to the irreducible error \u001b$\\sigma^2$.\n",
        "\n",
        "### Bias-Variance Decomposition\n",
        "At last, we extract a *new point* and evaluate its bias and variance, by relying on the  **bias-variance decomposition**.\n",
        "By considering the expected square error on an **unseen sample** $x$, we obtain:\n",
        "\\begin{equation}\n",
        "\\mathbb{E}_{\\mathcal{D},t}[(t - y(x))^2] = \\sigma^2 + Var_{\\mathcal{D}}[y(x)] + \\mathbb{E}_{\\mathcal{D}}[f(x)-y(x)]^2,\n",
        "\\end{equation}\n",
        "\n",
        "where we recall that f(x) is the real function generating the samples and the expectation is performed over the different realizations of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb8ObCXNZx6n"
      },
      "source": [
        "n_points = 1\n",
        "x_new = np.random.uniform(low=0, high=5, size=n_points)\n",
        "\n",
        "# Compute target and add noise\n",
        "t_new = fun(x_new)\n",
        "t_new_noisy = t_new + eps * np.random.randn(n_points, 1)\n",
        "\n",
        "# Compute features values\n",
        "x_enh_new = np.stack((np.ones(n_points), x_new, np.zeros(n_points)))\n",
        "phi_enh_new = np.stack((np.ones(n_points), x_new, x_new**2))\n",
        "\n",
        "# Predict target using the two models\n",
        "y_pred_lin = lin_coeff @ x_enh_new\n",
        "y_pred_qua = qua_coeff @ phi_enh_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MnaiUGGDFb"
      },
      "source": [
        "\\begin{equation}\n",
        "\\mathbb{E}[(t - y(x))^2]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHnyK5bnGHx7"
      },
      "source": [
        "# we are taking the mean w.r.t. the different datatsets\n",
        "error_lin = np.mean((t_new_noisy - y_pred_lin)**2) # square is inside here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOUHdWsNGEVX"
      },
      "source": [
        "\\begin{equation}\n",
        " \\mathbb{E}[f(x)-y(x)]^2\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiV4OHSsGHj6"
      },
      "source": [
        "bias_lin = np.mean(t_new - y_pred_lin)**2 # square is outside here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsgyWZd3GFXv"
      },
      "source": [
        "\\begin{equation}\n",
        " Var[y(x)]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSHDQW6hGCYX"
      },
      "source": [
        "variance_lin = np.var(y_pred_lin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgb1-Qk-GVND"
      },
      "source": [
        "\\begin{equation}\n",
        "\\sigma^2  = \\mathbb{E}[(t - y(x))^2] - Var[y(x)] - \\mathbb{E}[f(x)-y(x)]^2,\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ_PKg_lbsdd"
      },
      "source": [
        "var_t_lin = error_lin - variance_lin - bias_lin\n",
        "\n",
        "# we repeat the same formulas for the quadratic case\n",
        "error_qua = np.mean((t_new_noisy - y_pred_qua)**2)\n",
        "bias_qua = np.mean(t_new - y_pred_qua)**2\n",
        "variance_qua = np.var(y_pred_qua)\n",
        "var_t_qua = error_qua - variance_qua - bias_qua\n",
        "\n",
        "print('---Single Point---')\n",
        "print('Linear error: {}'.format(error_lin))\n",
        "print('Linear bias: {}'.format(bias_lin))\n",
        "print('Linear variance: {}'.format(variance_lin))\n",
        "print('Linear sigma: {} (unreliable)'.format(var_t_lin))\n",
        "\n",
        "print('---Single Point---')\n",
        "print('Quadratic error: {}'.format(error_qua))\n",
        "print('Quadratic bias: {}'.format(bias_qua))\n",
        "print('Quadratic variance: {}'.format(variance_qua))\n",
        "print('Quadratic sigma: {} (unreliable)'.format(var_t_qua))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ithutHmEdSFv"
      },
      "source": [
        "In this case, the estimation of the variance \u001b$\\sigma^2$ does not make sense since has been performed on a single sample.\n",
        "\n",
        "Similarly, we can compute the bias and variance of the two models by integrating **over the entire input space** $[0, 5]$ and have a more stable estimate of the bias, variance, and irreducible error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WY8T-9Cdako"
      },
      "source": [
        "n_points = 101\n",
        "\n",
        "x_new = np.linspace(0, 5, n_points)\n",
        "t_new = fun(x_new)\n",
        "t_new_noisy = t_new + eps * np.random.randn(n_points, 1).flatten()\n",
        "\n",
        "x_enh_new = np.stack([np.ones(n_points), x_new, np.zeros(n_points)])\n",
        "phi_enh_new = np.stack([np.ones(n_points), x_new, x_new**2])\n",
        "\n",
        "y_pred_lin = lin_coeff @ x_enh_new\n",
        "y_pred_qua = qua_coeff @ phi_enh_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50EJS2h1UQGd"
      },
      "source": [
        "y_pred_lin.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXR70vXKhSjV"
      },
      "source": [
        "# replicate the new points for each model\n",
        "error_lin = np.mean((np.tile(t_new_noisy, (n_repetitions,1)) - y_pred_lin)**2)\n",
        "bias_lin = np.mean(np.mean(np.tile(t_new, (n_repetitions,1)) - y_pred_lin, axis=0)**2) # I have to take the two means separately\n",
        "variance_lin = np.mean(np.var(y_pred_lin, axis=0)) # same here\n",
        "var_t_lin = error_lin - variance_lin - bias_lin\n",
        "\n",
        "error_qua = np.mean((np.tile(t_new_noisy, (n_repetitions,1)) - y_pred_qua)**2)\n",
        "bias_qua = np.mean(np.mean(np.tile(t_new, (n_repetitions,1)) - y_pred_qua, axis=0)**2)\n",
        "variance_qua = np.mean(np.var(y_pred_qua, axis=0))\n",
        "var_t_qua = error_qua - variance_qua - bias_qua\n",
        "\n",
        "print('---All dataset---')\n",
        "print('Linear error: {}'.format(error_lin))\n",
        "print('Linear bias: {}'.format(bias_lin))\n",
        "print('Linear variance: {}'.format(variance_lin))\n",
        "print('Linear sigma: {}'.format(var_t_lin))\n",
        "\n",
        "print('---All dataset--')\n",
        "print('Quadratic error: {}'.format(error_qua))\n",
        "print('Quadratic bias: {}'.format(bias_qua))\n",
        "print('Quadratic variance: {}'.format(variance_qua))\n",
        "print('Quadratic sigma: {}'.format(var_t_qua))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MVnuqFwyVsq"
      },
      "source": [
        "With this procedure we are able to check that the $L_2$ is generally able to reduce bias and variance sum, but it is not able to get a null irreducible error.\n",
        "\n",
        "\n",
        "**Remark 1.** In real situations we do not know the real model generating data. Therefore, the analysis we conducted before is not a viable option. Nonetheless, the phenomena we analysed are still valid, since in real cases we will have a single point in the parameter space, which will be distributed according to the distribution induced by the model we selected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta6hrOZA4zz2"
      },
      "source": [
        "## Dealing with the Bias-Variance Tradeoff\n",
        "\n",
        "In previous analysis, we made the assumption to know the real model, which is not an assumption\n",
        "we can do in real scenarios. To deal with the bias/variance dilemma in *real scenarios* we use a set of techniques able to take into account the bias/variance tradeoff without having the necessity to know the *real* model.\n",
        "\n",
        "At first, we want to generate a new dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYUfFxXx5SXE"
      },
      "source": [
        "n_tot = 40\n",
        "eps = 2\n",
        "\n",
        "def fun(x):\n",
        "  return (0.5 - x) * (5 - x) * (x - 3)\n",
        "\n",
        "x = np.random.uniform(low=0, high=5, size=(n_tot, 1))\n",
        "y = fun(x) + eps * np.random.randn(n_tot, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5askGOq565Q"
      },
      "source": [
        "We divide the dataset into:\n",
        "- Training set $X_{train}$, i.e., the data we will use to **learn** the model parameters;\n",
        "- Validation set $X_{vali}$, i.e., the data we will use to **select** the model;\n",
        "- Test set $X_{test}$, i.e., the data we will use to **evaluate** the performance of our model.\n",
        "\n",
        "Usually, we use a split proportional to 50%/25%/25% for the three sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubRHf3rq6SZk"
      },
      "source": [
        "n_train = 20\n",
        "n_vali = 10\n",
        "n_test = 10\n",
        "\n",
        "x_train, y_train = x[:n_train], y[:n_train]\n",
        "x_vali, y_vali = x[n_train:n_train + n_vali], y[n_train:n_train + n_vali]\n",
        "x_test, y_test = x[n_train + n_vali:], y[n_train + n_vali:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rd-CI458lky"
      },
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "plt.scatter(x_train, y_train, label='train')\n",
        "plt.scatter(x_vali, y_vali, label='validation')\n",
        "plt.scatter(x_test, y_test, label='test')\n",
        "\n",
        "plt.plot(np.linspace(0,5, 100), fun(np.linspace(0,5,100)))\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Target')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr3JlfaK9WF6"
      },
      "source": [
        "Here we are not required to shuffle the data since we generate the input at random, but, in general, it is a good practice to randomly rearrange the data before splitting, since some ordering might be induced by those who provided you the data (e.g., enforced by some query on a database).\n",
        "Here, we use:\n",
        "- Hypothesis space: $y_n = f(x_n,w) = \\sum_{k=0}^o x_n^k w_k$;\n",
        "- Loss measure: $RSS(w) = \\sum_n(y_n - t_n)^2$\n",
        "- Optimization method: Least Square (LS) method;\n",
        "\n",
        "and we want to select among polynomial models with order $o \\in \\{ 0, \\dots, 9 \\}$. If we compute the training error on different models we have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouEifNQXNke4"
      },
      "source": [
        "# We employ this useful function from scikit-learn to produce polynomial features\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "degree = 3\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "x_train_tr = poly.fit_transform(x_train)\n",
        "print(x_train_tr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wF9nBBB_IeQ"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "models = [] # models are stored here\n",
        "models_mse = [] # mse are stored here\n",
        "\n",
        "for degree in range(0, 10):\n",
        "  # fit a linear model for each of the selected polynomial orders\n",
        "  lin_model = linear_model.LinearRegression()\n",
        "  poly = PolynomialFeatures(degree=degree)\n",
        "  x_train_tr = poly.fit_transform(x_train)\n",
        "  lin_model.fit(x_train_tr, y_train)\n",
        "\n",
        "  # measure model performance\n",
        "  y_train_pred = lin_model.predict(x_train_tr)\n",
        "  mse = mean_squared_error(y_train, y_train_pred)\n",
        "\n",
        "  # store\n",
        "  models.append(lin_model)\n",
        "  models_mse.append(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyhBM5oV_wdw"
      },
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "plt.plot(models_mse)\n",
        "plt.title('Only Training')\n",
        "plt.grid()\n",
        "plt.xlabel('Order')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZB9cp7NCRW5"
      },
      "source": [
        "We can see that this model does not equally perform on a test set:\n",
        "with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkr75445CB5N"
      },
      "source": [
        "models_mse_test = []\n",
        "for degree in range(0, 10):\n",
        "  # load the fitted model\n",
        "  lin_model = models[degree]\n",
        "\n",
        "  # compute polynomial features for the test\n",
        "  poly = PolynomialFeatures(degree=degree)\n",
        "  x_test_tr = poly.fit_transform(x_test)\n",
        "\n",
        "  # measure model performance on TEST\n",
        "  y_test_pred = lin_model.predict(x_test_tr)\n",
        "  mse = mean_squared_error(y_test, y_test_pred)\n",
        "  models_mse_test.append(mse)\n",
        "\n",
        "plt.figure(figsize=(16,9))\n",
        "plt.plot(models_mse, label='Training MSE')\n",
        "plt.plot(models_mse_test, label='Test MSE')\n",
        "plt.title('Training and test error')\n",
        "plt.grid()\n",
        "plt.xlabel('Order')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq27RbWYHYln"
      },
      "source": [
        "To deal with this problem we will use a validation set and tune the order o of the polynomial on that set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnqTJfn_DUEK"
      },
      "source": [
        "models_mse_vali = []\n",
        "for degree in range(0, 10):\n",
        "  # load the fitted model\n",
        "  lin_model = models[degree]\n",
        "\n",
        "  # compute polynomial features for the test\n",
        "  poly = PolynomialFeatures(degree=degree)\n",
        "  x_vali_tr = poly.fit_transform(x_vali)\n",
        "  y_vali_pred = lin_model.predict(x_vali_tr)\n",
        "\n",
        "  # measure model performance on TEST\n",
        "  mse = mean_squared_error(y_vali, y_vali_pred)\n",
        "  models_mse_vali.append(mse)\n",
        "\n",
        "plt.figure(figsize=(16,9))\n",
        "plt.plot(models_mse, label='Training MSE')\n",
        "plt.plot(models_mse_test, label='Test MSE')\n",
        "plt.plot(models_mse_vali, label='Validation MSE')\n",
        "\n",
        "best_model = np.argmin(models_mse_vali)\n",
        "plt.axvline(best_model)\n",
        "plt.title('Training and test error')\n",
        "plt.grid()\n",
        "plt.xlabel('Order')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcYR3nPfI61y"
      },
      "source": [
        "However, the results of this procedure are strongly dependent on the validation set we used. Moreover, we are not using some of the samples during the training, which could improve the model accuracy.\n",
        "\n",
        "To better exploit the available data, we could resort to the use of **cross-validation**. This way, we have to join the Training and Validation set and divide it in k equally long partitions and use sequentially $k-1$ of them as training set and the remaining one as validation set. In the end, we average the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi7SHtj_I6Is"
      },
      "source": [
        "k_fold = 4\n",
        "n_cross = n_train + n_vali\n",
        "x_cross, y_cross = x[:n_cross], y[:n_cross]\n",
        "cross_indices = list(range(len(x_cross)))\n",
        "MSE_cross = np.zeros(10)\n",
        "for degree in range(10):\n",
        "  # divide data\n",
        "  for k in range(k_fold):\n",
        "    start_vali = int(np.round(n_cross * k / k_fold)) # start of the validation fold\n",
        "    end_vali = int(np.round(n_cross * (k+1) / k_fold)) # end of the validation fold\n",
        "\n",
        "    vali_indices = list(range(int(start_vali), int(end_vali)))\n",
        "    train_indices = [i for i in cross_indices if i not in vali_indices] # train on all the other indices\n",
        "\n",
        "    x_fold_train = x_cross[train_indices]\n",
        "    y_fold_train = y_cross[train_indices]\n",
        "\n",
        "    x_fold_vali = x_cross[vali_indices]\n",
        "    y_fold_vali = y_cross[vali_indices]\n",
        "\n",
        "    # train a model with polynomial feature\n",
        "    lin_model = linear_model.LinearRegression()\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    x_train_tr = poly.fit_transform(x_fold_train)\n",
        "    x_vali_tr = poly.fit_transform(x_fold_vali)\n",
        "    lin_model.fit(x_train_tr, y_fold_train)\n",
        "\n",
        "    # measure cross_validation error\n",
        "    y_vali_pred = lin_model.predict(x_vali_tr)\n",
        "    MSE = mean_squared_error(y_fold_vali, y_vali_pred)\n",
        "    MSE_cross[degree] += MSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVaL8u9PLeXh"
      },
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "plt.plot(models_mse, label='Training MSE')\n",
        "plt.plot(models_mse_test, label='Test MSE')\n",
        "plt.plot(models_mse_vali, label='Validation MSE')\n",
        "plt.plot(MSE_cross/k_fold, label='Cross-Validation MSE')\n",
        "\n",
        "best_model = np.argmin(MSE_cross)\n",
        "plt.axvline(best_model)\n",
        "plt.title('Training and test error')\n",
        "plt.grid()\n",
        "plt.xlabel('Order')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzCt9aBr7yce"
      },
      "source": [
        "The value of the error in the optimal point may be greater than the actual error we have on a test\n",
        "set.\n",
        "If we want to perform the Leave One Out (LOO), we just have to modify the number of folds we consider in the cross-validation procedure, i.e., $\\text{k_fold}\n",
        "= 30$.\n",
        "\n",
        "This kind of procedure is less biased than cross-validation, but requires more computational time."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boostrap confidence intervals on test error"
      ],
      "metadata": {
        "id": "H-UVcLsYfQug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  lin_model = models[5]\n",
        "\n",
        "  # compute polynomial features for the test\n",
        "  poly = PolynomialFeatures(degree=5)\n",
        "  x_test_tr = poly.fit_transform(x_test)\n",
        "\n",
        "  # measure model performance on TEST\n",
        "  y_test_pred = lin_model.predict(x_test_tr)\n",
        "  mse = mean_squared_error(y_test, y_test_pred)\n",
        "  print(mse)"
      ],
      "metadata": {
        "id": "wT1_hEgKcSdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = x_test_tr.shape[0]\n",
        "M = 1000 #100 resamples\n",
        "confidence = 0.9\n",
        "boot_mses = []\n",
        "for _ in range(M):\n",
        "  ii = np.random.choice(N, size=N) #with replacement by default\n",
        "  x_boot = x_test_tr[ii,:]\n",
        "  y_boot = y_test[ii,:]\n",
        "  y_boot_pred = lin_model.predict(x_boot)\n",
        "  boot_mses.append(mean_squared_error(y_boot, y_boot_pred))\n",
        "boot_mses = np.array(boot_mses)\n",
        "\n",
        "low = np.quantile(boot_mses, (1 - confidence)/2) #5th percentile\n",
        "high = np.quantile(boot_mses, (1+confidence)/2) #95th percentile\n",
        "print(\"Test MSE is in [{}, {}] with {}% confidence\".format(low, high, confidence*100))"
      ],
      "metadata": {
        "id": "PAkvDBIYdSWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3jAzFFfyPxJ"
      },
      "source": [
        "# Homeworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRN3p5_9TSRr"
      },
      "source": [
        "## Complexity-Adjusted Model Evaluation\n",
        "Compute the $C_p$, $AIC$, $BIC$ and adjusted-$R2$ indexes for the dataset generated in the last part of the lecture (training + validation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iLE0dJBzexK"
      },
      "source": [
        "## Implementing Adaboost\n",
        "Implement the AdaBoost meta-algorithm for linear regression and by using a weighted re-sampling technique to create each model on the data we considered in the last part of the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMe1RqvayTNo"
      },
      "source": [
        "## Bootstrapping from generated data: Naive Bayes\n",
        "\n",
        "Let's go back to the previous exercise session. Using Naive Bayes we know how\n",
        " to generate new samples, thus we can arbitrarily enlarge our training set.\n",
        "\n",
        "What happens if we train a new model with also the generated data?\n",
        "\n",
        "Try to confirm your intuitions with this experiment:\n",
        "- divide the dataset in train and test\n",
        "- train a Naive Bayes model with the training part\n",
        "- generate new samples with the generative model\n",
        "- train another Naive Bayes model with the enlarged training set\n",
        "- test both models on the test set\n",
        "- compute bias, variance and irreducibile error, using the formula when needed"
      ]
    }
  ]
}